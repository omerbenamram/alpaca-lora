{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import jsonlines\n",
    "import tiktoken\n",
    "import textwrap\n",
    "import torch\n",
    "import random\n",
    "import requests\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from peft import PeftModel\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, GenerationConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.prompter import Prompter\n",
    "from utils.callbacks import Iteratorize, Stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "base_model = \"huggyllama/llama-7b\"\n",
    "lora_weights = \"./lora-hi-7b-lora-16/checkpoint-590/adapter_model/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b746774273b8424a9d58f19ef3d6d01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompter = Prompter(\"hi_detailed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/omer/.cache/huggingface/datasets/json/default-11680bcd564fc7b9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b1f62d8a524450a5edccc7e95823ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"./hi_short_test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_callback(callback=None, **kwargs):\n",
    "    kwargs.setdefault(\"stopping_criteria\", transformers.StoppingCriteriaList())\n",
    "    kwargs[\"stopping_criteria\"].append(Stream(callback_func=callback))\n",
    "    with torch.no_grad():\n",
    "        model.generate(**kwargs)\n",
    "\n",
    "def generate_with_streaming(**kwargs):\n",
    "    return Iteratorize(generate_with_callback, kwargs, callback=None)\n",
    "\n",
    "def gen(generate_params):\n",
    "    with generate_with_streaming(**generate_params) as generator:\n",
    "        for output in generator:\n",
    "            new_tokens = len(output) - len(input_ids[0])\n",
    "            decoded_output = tokenizer.decode(output)\n",
    "\n",
    "            # if output[-1] in [tokenizer.eos_token_id]:\n",
    "            #     break\n",
    "\n",
    "            # yield decoded_output.split(\"\\n\\n===\\n\\nGrey: \")[1].strip()\n",
    "            yield decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a summary of a conversation between grey and brady, paired with an exchange between them. Write a response that best completes what grey has to say.\n",
      "\n",
      "Summary: Grey and Brady discuss the changes to YouTube's Partner Program and the impact it may have on creators. Grey believes that the changes are an attempt to mitigate the effects of the adpocalypse and to ensure that YouTube remains a viable platform for advertisers.\n",
      "\n",
      "Tone:  The tone of the conversation is concerned.\n",
      "\n",
      "###\n",
      "\n",
      "Brady: No, I don't. I think it's really dangerous.\n",
      "Grey: I'm kind of curious though, as someone who has worked in TV, can you articulate why you think there's a difference between YouTube and TV? Because I think someone can make the argument that it's like, well, it all comes through your iPhone now and they're just apps and what's the difference now?\n",
      "Brady: I wish you hadn't asked that because I find it really hard to answer.\n",
      "Grey: But it is, right? I think that's an argument that doesn't have an immediately obviously good answer as to why is Netflix different from TV? Well, it comes over the internet. It's like, well, your TV comes over the internet.\n",
      "Brady: Netflix is probably more heavily regulated too. I don't know.\n",
      "Grey:\n",
      "\n",
      "===\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datum = dataset[\"train\"][1006]\n",
    "prompt = prompter.generate_prompt(\n",
    "    datum[\"input\"]\n",
    ")\n",
    "print(prompt)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.3,\n",
    "    top_p=0.75,\n",
    "    top_k=50,\n",
    "    num_beams=4,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "generate_params = {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"generation_config\": generation_config,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"output_scores\": True,\n",
    "    \"repetition_penalty\": 4.0,\n",
    "    \"length_penalty\":0.3,\n",
    "    \"max_new_tokens\": 2048,\n",
    "    \"renormalize_logits\": True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yeah, maybe that's part of it. Maybe that's part of it. And also, again, this is something where I feel like we need\n",
      "some data here. Like, how much money does YouTube earn versus how much money does Netflix earn? That would be very\n",
      "interesting to know.\n"
     ]
    }
   ],
   "source": [
    "for output in gen(generate_params):\n",
    "    clear_output()\n",
    "    print(\"\\n\".join(textwrap.wrap(prompter.get_response(output), 120)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
